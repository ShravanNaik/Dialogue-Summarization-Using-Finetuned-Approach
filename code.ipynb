{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6766d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers==4.36.2\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install datasets==2.15.0\n",
    "!pip install peft==0.7.1\n",
    "!pip install evaluate==0.4.1\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b873a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Hugging Face token from environment variables\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System: {platform.system()} {platform.machine()}\")\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"🚀 MPS (Metal Performance Shaders) available - using GPU acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"🔥 CUDA available - using NVIDIA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"💻 Using CPU - training will be slower but still functional\")\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b64ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    \"WANDB_PROJECT\": \"Supervised-fine-tune-models\",\n",
    "    \"WANDB_NOTES\": \"MacBook PHI-2 fine-tuning\",\n",
    "    \"WANDB_NAME\": \"sft-phi2-dialogsum-macbook\",\n",
    "    \"MODEL_NAME\": \"microsoft/phi-2\",\n",
    "    \"DATASET_NAME\": \"neil-code/dialogsum-test\"\n",
    "}\n",
    "\n",
    "for key, value in experiment_config.items():\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "original_dataset = load_dataset(os.getenv('DATASET_NAME'))\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Use smaller subsets for faster training on MacBook\n",
    "SAMPLE_SIZE = 50  # Reduced for MacBook performance\n",
    "\n",
    "train_data = original_dataset['train'].select(range(SAMPLE_SIZE))\n",
    "validation_data = original_dataset['validation'].select(range(SAMPLE_SIZE))\n",
    "test_data = original_dataset['test'].select(range(SAMPLE_SIZE))\n",
    "\n",
    "dataset_splits = {\n",
    "    'train': train_data,\n",
    "    'validation': validation_data,\n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "print(f\"Dataset prepared with {SAMPLE_SIZE} samples each:\")\n",
    "for split, data in dataset_splits.items():\n",
    "    print(f\"  {split}: {len(data)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98069027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# see https://github.com/huggingface/transformers/issues/18388 for description about padding\n",
    "tokenizer=AutoTokenizer.from_pretrained(\n",
    "    os.getenv('MODEL_NAME'),\n",
    "    padding_side='left',\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "phi2_tokenizer = tokenizer\n",
    "print(f\"✅ Tokenizer initialized with vocab size: {len(phi2_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b9896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from transformers import set_seed, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "def format_prompt(sample):\n",
    "    \"\"\"Create structured prompt from dialogue and summary\"\"\"\n",
    "    INTRO = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION = \"### Instruct: Summarize the below conversation.\"\n",
    "    OUTPUT_MARKER = \"### Output:\"\n",
    "    END_MARKER = \"### End\"\n",
    "    \n",
    "    # Build complete prompt\n",
    "    prompt_parts = [\n",
    "        f\"\\n{INTRO}\",\n",
    "        f\"{INSTRUCTION}\",\n",
    "        f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None,\n",
    "        f\"{OUTPUT_MARKER}\\n{sample['summary']}\",\n",
    "        f\"{END_MARKER}\"\n",
    "    ]\n",
    "    \n",
    "    # Filter out None parts and join\n",
    "    formatted_prompt = \"\\n\\n\".join([part for part in prompt_parts if part])\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "    return sample\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length):\n",
    "    \"\"\"Tokenize the text data\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer, max_length=1024):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    print(\"Applying prompt formatting...\")\n",
    "    formatted_dataset = dataset.map(format_prompt)\n",
    "    \n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenize_fn = partial(tokenize_function, tokenizer=tokenizer, max_length=max_length)\n",
    "    tokenized_dataset = formatted_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
    "    )\n",
    "    \n",
    "    # Filter out sequences that are too long\n",
    "    filtered_dataset = tokenized_dataset.filter(\n",
    "        lambda x: len(x[\"input_ids\"]) < max_length\n",
    "    )\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    shuffled_dataset = filtered_dataset.shuffle(seed=RANDOM_SEED)\n",
    "    return shuffled_dataset\n",
    "\n",
    "# Set up data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=phi2_tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70584bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def load_model_for_macbook(model_name, device):\n",
    "    \"\"\"Load PHI-2 model optimized for MacBook\"\"\"\n",
    "    print(f\"Loading model on {device}...\")\n",
    "    \n",
    "    if device.type == \"mps\":\n",
    "        # Optimized for Apple Silicon - remove device_map=\"auto\" for Phi models\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        # Manually move to MPS device\n",
    "        model = model.to(device)\n",
    "    elif device.type == \"cuda\":\n",
    "        # NVIDIA GPU settings\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        # CPU fallback\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ Model loaded successfully on {model.device}\")\n",
    "    return model\n",
    "\n",
    "# Load the base model\n",
    "base_model = load_model_for_macbook(os.getenv('MODEL_NAME'), device)\n",
    "\n",
    "# Get max sequence length\n",
    "max_seq_length = getattr(base_model.config, 'max_position_embeddings', 2048)\n",
    "print(f\"Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc83bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing datasets...\")\n",
    "\n",
    "train_dataset = preprocess_dataset(\n",
    "    dataset_splits['train'], \n",
    "    phi2_tokenizer, \n",
    "    max_seq_length\n",
    ")\n",
    "\n",
    "eval_dataset = preprocess_dataset(\n",
    "    dataset_splits['validation'], \n",
    "    phi2_tokenizer, \n",
    "    max_seq_length\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7722d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA configuration optimized for MacBook\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Reduced rank for MacBook memory efficiency\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        'q_proj', 'k_proj', 'v_proj', 'dense', 'fc1', 'fc2'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print(\"✅ LoRA configuration applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ff30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import time\n",
    "\n",
    "def get_training_args_for_device(device):\n",
    "    \"\"\"Get optimized training arguments based on device\"\"\"\n",
    "    \n",
    "    # Determine if wandb should be used\n",
    "    use_wandb = os.getenv('WANDB_API_KEY') and os.getenv('WANDB_DISABLED') != 'true'\n",
    "    report_to = \"wandb\" if use_wandb else None\n",
    "    \n",
    "    if device.type == \"mps\":\n",
    "        # Apple Silicon optimized settings\n",
    "        return TrainingArguments(\n",
    "            output_dir=os.getenv(\"WANDB_NAME\"),\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=True,\n",
    "            num_train_epochs=2,\n",
    "            max_steps=50,  # Reduced for MacBook\n",
    "            warmup_steps=10,\n",
    "            learning_rate=5e-5,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=5,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            fp16=False,  # MPS doesn't support fp16 well\n",
    "            dataloader_pin_memory=False,\n",
    "            dataloader_num_workers=0,  # Avoid multiprocessing issues on macOS\n",
    "            report_to=report_to,\n",
    "            run_name=os.getenv(\"WANDB_NAME\"),\n",
    "        )\n",
    "    elif device.type == \"cuda\":\n",
    "        # NVIDIA GPU settings\n",
    "        return TrainingArguments(\n",
    "            output_dir=os.getenv(\"WANDB_NAME\"),\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=True,\n",
    "            num_train_epochs=2,\n",
    "            max_steps=50,\n",
    "            warmup_steps=10,\n",
    "            learning_rate=5e-5,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=5,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=25,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            fp16=True,\n",
    "            report_to=report_to,\n",
    "            run_name=os.getenv(\"WANDB_NAME\"),\n",
    "        )\n",
    "    else:\n",
    "        # CPU settings\n",
    "        return TrainingArguments(\n",
    "            output_dir=os.getenv(\"WANDB_NAME\"),\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=2,\n",
    "            num_train_epochs=1,  # Reduced for CPU\n",
    "            max_steps=20,\n",
    "            warmup_steps=5,\n",
    "            learning_rate=5e-5,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=5,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=10,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=20,\n",
    "            save_total_limit=1,\n",
    "            dataloader_num_workers=0,\n",
    "            report_to=report_to,\n",
    "            run_name=os.getenv(\"WANDB_NAME\"),\n",
    "        )\n",
    "\n",
    "# Get training arguments\n",
    "training_args = get_training_args_for_device(device)\n",
    "\n",
    "# Disable cache for training\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(f\"✅ Trainer configured for {device} with batch size {training_args.per_device_train_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3771fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases if API key is available\n",
    "if os.getenv('WANDB_API_KEY'):\n",
    "    wandb.login(key=os.getenv('WANDB_API_KEY'))\n",
    "    wandb.init(\n",
    "        project=os.getenv('WANDB_PROJECT'),\n",
    "        name=os.getenv('WANDB_NAME'),\n",
    "        notes=os.getenv('WANDB_NOTES'),\n",
    "        config={\n",
    "            \"model_name\": os.getenv('MODEL_NAME'),\n",
    "            \"dataset_name\": os.getenv('DATASET_NAME'),\n",
    "            \"device\": str(device),\n",
    "            \"sample_size\": 50,\n",
    "            \"random_seed\": 42,\n",
    "        }\n",
    "    )\n",
    "    print(\"✅ Weights & Biases initialized successfully!\")\n",
    "else:\n",
    "    print(\"⚠️ WANDB_API_KEY not found - training without W&B logging\")\n",
    "    # Disable wandb in training args\n",
    "    os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "\n",
    "training_duration = end_time - start_time\n",
    "print(f\"✅ Training completed in {training_duration:.2f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = f\"./{os.getenv('WANDB_NAME')}\"\n",
    "trainer.save_model(model_save_path)\n",
    "phi2_tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model saved locally to: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_model_for_inference(base_model_name, peft_model_path, device):\n",
    "    \"\"\"Load the fine-tuned model for inference\"\"\"\n",
    "    # Load base model\n",
    "    if device.type == \"mps\":\n",
    "        # For MPS, don't use device_map=\"auto\"\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        base_model = base_model.to(device)\n",
    "    else:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32,\n",
    "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "    return model\n",
    "\n",
    "# Load tokenizer for inference - use slow tokenizer to avoid BOS token issue\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_save_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False  # Use slow tokenizer to avoid BOS token error\n",
    ")\n",
    "inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "\n",
    "# Load fine-tuned model\n",
    "inference_model = load_model_for_inference(\n",
    "    os.getenv('MODEL_NAME'),\n",
    "    model_save_path,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"Model loaded for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, prompt, max_new_tokens=150, device=device):\n",
    "    \"\"\"Generate summary using the fine-tuned model\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    \n",
    "    # Move to device\n",
    "    if device.type != \"cpu\":\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate with more stable parameters\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Use greedy decoding for stability\n",
    "                temperature=1.0,  # Reset to default\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,  # Prevent repetition\n",
    "                use_cache=True,\n",
    "            )\n",
    "        except RuntimeError as e:\n",
    "            if \"probability tensor\" in str(e):\n",
    "                # Fallback to beam search if sampling fails\n",
    "                print(\"⚠️ Sampling failed, trying beam search...\")\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=2,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.1,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = dataset_splits['test'][0]\n",
    "test_dialogue = test_sample['dialogue']\n",
    "ground_truth = test_sample['summary']\n",
    "\n",
    "# Create test prompt\n",
    "test_prompt = f\"Instruct: Summarize the following conversation.\\n{test_dialogue}\\nOutput:\\n\"\n",
    "\n",
    "# Generate summary\n",
    "print(\"Generating summary...\")\n",
    "generated_output = generate_summary(\n",
    "    inference_model, \n",
    "    inference_tokenizer, \n",
    "    test_prompt, \n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "# Extract the summary part\n",
    "try:\n",
    "    generated_summary = generated_output.split(\"Output:\\n\")[1].split(\"### End\")[0].strip()\n",
    "except:\n",
    "    generated_summary = generated_output\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"INPUT DIALOGUE:\")\n",
    "print(test_dialogue)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GROUND TRUTH SUMMARY:\")\n",
    "print(ground_truth)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED SUMMARY:\")\n",
    "print(generated_summary)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68054921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix wandb socket issues by properly finishing any active runs\n",
    "try:\n",
    "    import wandb\n",
    "    if wandb.run is not None:\n",
    "        print(\"🔄 Finishing active wandb run...\")\n",
    "        wandb.finish()\n",
    "        print(\"✅ Wandb run finished\")\n",
    "    \n",
    "    # Disable wandb to prevent socket errors\n",
    "    os.environ['WANDB_DISABLED'] = 'true'\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "    print(\"🚫 Wandb disabled to prevent socket errors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error handling wandb: {e}\")\n",
    "    # Force disable wandb\n",
    "    os.environ['WANDB_DISABLED'] = 'true'\n",
    "    os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# Clear any wandb cache/state\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(\"🔧 Wandb issues resolved - ready to continue testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ffbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, tokenizer, test_dataset, num_samples=10):\n",
    "    \"\"\"Evaluate model performance with metrics\"\"\"\n",
    "    \n",
    "    print(\"📊 MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    import time\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    metrics = defaultdict(list)\n",
    "    \n",
    "    for i in range(min(num_samples, len(test_dataset))):\n",
    "        test_sample = test_dataset[i]\n",
    "        test_dialogue = test_sample['dialogue']\n",
    "        ground_truth = test_sample['summary']\n",
    "        \n",
    "        test_prompt = f\"Instruct: Summarize the following conversation.\\n{test_dialogue}\\nOutput:\\n\"\n",
    "        \n",
    "        try:\n",
    "            # Measure generation time\n",
    "            start_time = time.time()\n",
    "            generated_output = generate_summary(\n",
    "                model, \n",
    "                tokenizer, \n",
    "                test_prompt, \n",
    "                max_new_tokens=100\n",
    "            )\n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            # Extract summary\n",
    "            try:\n",
    "                generated_summary = generated_output.split(\"Output:\\n\")[1].split(\"### End\")[0].strip()\n",
    "                if not generated_summary:\n",
    "                    generated_summary = generated_output.strip()\n",
    "            except:\n",
    "                generated_summary = generated_output.strip()\n",
    "            \n",
    "            # Calculate basic metrics\n",
    "            dialogue_length = len(test_dialogue.split())\n",
    "            summary_length = len(generated_summary.split())\n",
    "            ground_truth_length = len(ground_truth.split())\n",
    "            compression_ratio = dialogue_length / summary_length if summary_length > 0 else 0\n",
    "            \n",
    "            # Store metrics\n",
    "            metrics['generation_time'].append(generation_time)\n",
    "            metrics['dialogue_length'].append(dialogue_length)\n",
    "            metrics['summary_length'].append(summary_length)\n",
    "            metrics['ground_truth_length'].append(ground_truth_length)\n",
    "            metrics['compression_ratio'].append(compression_ratio)\n",
    "            \n",
    "            print(f\"Sample {i+1}: ⏱️{generation_time:.2f}s | 📝{dialogue_length}→{summary_length} words | 🗜️{compression_ratio:.1f}x\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Sample {i+1} failed: {str(e)}\")\n",
    "    \n",
    "    # Calculate and display averages\n",
    "    if metrics['generation_time']:\n",
    "        print(f\"\\n📈 PERFORMANCE METRICS\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Average generation time: {sum(metrics['generation_time'])/len(metrics['generation_time']):.2f} seconds\")\n",
    "        print(f\"Average dialogue length: {sum(metrics['dialogue_length'])/len(metrics['dialogue_length']):.1f} words\")\n",
    "        print(f\"Average generated summary: {sum(metrics['summary_length'])/len(metrics['summary_length']):.1f} words\")\n",
    "        print(f\"Average ground truth: {sum(metrics['ground_truth_length'])/len(metrics['ground_truth_length']):.1f} words\")\n",
    "        print(f\"Average compression ratio: {sum(metrics['compression_ratio'])/len(metrics['compression_ratio']):.1f}x\")\n",
    "        print(f\"Total samples processed: {len(metrics['generation_time'])}\")\n",
    "        \n",
    "        # Generation speed\n",
    "        words_per_second = sum(metrics['summary_length']) / sum(metrics['generation_time'])\n",
    "        print(f\"Generation speed: {words_per_second:.1f} words/second\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run performance evaluation\n",
    "performance_metrics = evaluate_model_performance(\n",
    "    inference_model, \n",
    "    inference_tokenizer, \n",
    "    dataset_splits['test'], \n",
    "    num_samples=8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
