{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b873a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Hugging Face token from environment variables\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"ðŸš€ MPS (Metal Performance Shaders) available - using GPU acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"ðŸ”¥ CUDA available - using NVIDIA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"ðŸ’» Using CPU - training will be slower but still functional\")\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f21817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "original_dataset = load_dataset(os.getenv('DATASET_NAME'))\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Use smaller subsets for faster training on MacBook\n",
    "SAMPLE_SIZE = 50  # Reduced for MacBook performance\n",
    "\n",
    "train_data = original_dataset['train'].select(range(SAMPLE_SIZE))\n",
    "validation_data = original_dataset['validation'].select(range(SAMPLE_SIZE))\n",
    "test_data = original_dataset['test'].select(range(SAMPLE_SIZE))\n",
    "\n",
    "dataset_splits = {\n",
    "    'train': train_data,\n",
    "    'validation': validation_data,\n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "print(f\"Dataset prepared with {SAMPLE_SIZE} samples each:\")\n",
    "for split, data in dataset_splits.items():\n",
    "    print(f\"  {split}: {len(data)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f098a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from transformers import set_seed, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def load_model_for_macbook(model_name, device):\n",
    "    \"\"\"Load PHI-2 model optimized for MacBook\"\"\"\n",
    "    print(f\"Loading model on {device}...\")\n",
    "    \n",
    "    if device.type == \"mps\":\n",
    "        # Optimized for Apple Silicon - remove device_map=\"auto\" for Phi models\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        # Manually move to MPS device\n",
    "        model = model.to(device)\n",
    "    elif device.type == \"cuda\":\n",
    "        # NVIDIA GPU settings\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    else:\n",
    "        # CPU fallback\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ… Model loaded successfully on {model.device}\")\n",
    "    return model\n",
    "\n",
    "# Load the base model\n",
    "base_model = load_model_for_macbook(os.getenv('MODEL_NAME'), device)\n",
    "\n",
    "# Get max sequence length\n",
    "max_seq_length = getattr(base_model.config, 'max_position_embeddings', 2048)\n",
    "print(f\"Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1535c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# LoRA configuration optimized for MacBook\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Reduced rank for MacBook memory efficiency\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        'q_proj', 'k_proj', 'v_proj', 'dense', 'fc1', 'fc2'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ… LoRA configuration applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "\n",
    "training_duration = end_time - start_time\n",
    "print(f\"âœ… Training completed in {training_duration:.2f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_model_for_inference(base_model_name, peft_model_path, device):\n",
    "    \"\"\"Load the fine-tuned model for inference\"\"\"\n",
    "    # Load base model\n",
    "    if device.type == \"mps\":\n",
    "        # For MPS, don't use device_map=\"auto\"\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        base_model = base_model.to(device)\n",
    "    else:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32,\n",
    "            device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "    return model\n",
    "\n",
    "# Load tokenizer for inference - use slow tokenizer to avoid BOS token issue\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_save_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False  # Use slow tokenizer to avoid BOS token error\n",
    ")\n",
    "inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "\n",
    "# Load fine-tuned model\n",
    "inference_model = load_model_for_inference(\n",
    "    os.getenv('MODEL_NAME'),\n",
    "    model_save_path,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"Model loaded for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = dataset_splits['test'][0]\n",
    "test_dialogue = test_sample['dialogue']\n",
    "ground_truth = test_sample['summary']\n",
    "\n",
    "# Create test prompt\n",
    "test_prompt = f\"Instruct: Summarize the following conversation.\\n{test_dialogue}\\nOutput:\\n\"\n",
    "\n",
    "# Generate summary\n",
    "print(\"Generating summary...\")\n",
    "generated_output = generate_summary(\n",
    "    inference_model, \n",
    "    inference_tokenizer, \n",
    "    test_prompt, \n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "# Extract the summary part\n",
    "try:\n",
    "    generated_summary = generated_output.split(\"Output:\\n\")[1].split(\"### End\")[0].strip()\n",
    "except:\n",
    "    generated_summary = generated_output\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"INPUT DIALOGUE:\")\n",
    "print(test_dialogue)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GROUND TRUTH SUMMARY:\")\n",
    "print(ground_truth)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED SUMMARY:\")\n",
    "print(generated_summary)\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
